---
# This template guides you to create a spark cluster automatically.
# Except the fields that labeled optional all other fields are required.
boo:
  oneops_host: https://web.bfd.dev.cloud.wal-mart.com/
  organization: 'bfd' # OneOps org name
  api_key: '' # copy from OneOps UI->profile->authentication->API Token
  email: ''
  environment_name: 'env-spark' # All the deployment in this template adds to this environment.
  ip_output: 'json'

assembly:
  name: 'boo-test-spark-user' # Defined by user
  auto_gen: false # If turn on, then this tool will generate random assembly name for you.
variables: # Optional, global variables.
  
platforms: # In OneOps design.
  spark: # This is as your platform name as well
    deploy_order: 1
    pack: oneops/spark
    pack_version: '1'
    components: # Optional
      spark:
        spark_download_location: 'custom'
        spark_custom_download: ''
      hadoop-yarn:
        swift_password: '' 
        hive_db_password: '' 
        additional_libraries: ''
        yarn_tarball: '' 
        swift_username: ''
        swift_url: ''
        swift_tenant: ''
        s3_endpoint: ''
        hive_connect_url: ''
        hive_tarball_url: '' 
        hive_standalone_namenode: ''
        extra_core_site: '<property>\r\n  <name>fs.swift.service.strati-cdc-demo.auth.endpoint.prefix</name>\r\n
 <value>${fs.swift.service.strati.auth.endpoint.prefix}</value>\r\n</property>\r\n\r\n<property>\r\n
 <name>fs.swift.service.strati-cdc-demo.auth.url</name>\r\n  <value>${fs.swift.service.strati.auth.url}</value>\r\n</property>\r\n\r\n<property>\r\n
 <name>fs.swift.service.strati-cdc-demo.username</name>\r\n  <value>${fs.swift.service.strati.username}</value>\r\n</property>\r\n\r\n<property>\r\n
 <name>fs.swift.service.strati-cdc-demo.password</name>\r\n  <value>${fs.swift.service.strati.password}</value>\r\n</property>\r\n\r\n<property>\r\n
 <name>fs.swift.service.strati-cdc-demo.public</name>\r\n  <value>${fs.swift.service.strati.public}</value>\r\n</property>\r\n\r\n<property>\r\n
 <name>fs.swift.service.strati-cdc-demo.region</name>\r\n  <value>RegionOne</value>\r\n</property>\r\n\r\n<property>\r\n
 <name>fs.swift.service.strati-cdc-demo.tenant</name>\r\n  <value>${fs.swift.service.strati.tenant}</value>\r\n</property>\r\n'
      hadoop-yarn-master:
        swift_password: '' 
        hive_db_password: '' 
        additional_libraries: ''
        yarn_tarball: '' 
        swift_username: ''
        swift_url: ''
        swift_tenant: ''
        s3_endpoint: ''
        hive_connect_url: ''
        hive_tarball_url: '' 
        hive_standalone_namenode: ''
        extra_core_site: '<property>\r\n  <name>fs.swift.service.strati-cdc-demo.auth.endpoint.prefix</name>\r\n
 <value>${fs.swift.service.strati.auth.endpoint.prefix}</value>\r\n</property>\r\n\r\n<property>\r\n
 <name>fs.swift.service.strati-cdc-demo.auth.url</name>\r\n  <value>${fs.swift.service.strati.auth.url}</value>\r\n</property>\r\n\r\n<property>\r\n
 <name>fs.swift.service.strati-cdc-demo.username</name>\r\n  <value>${fs.swift.service.strati.username}</value>\r\n</property>\r\n\r\n<property>\r\n
 <name>fs.swift.service.strati-cdc-demo.password</name>\r\n  <value>${fs.swift.service.strati.password}</value>\r\n</property>\r\n\r\n<property>\r\n
 <name>fs.swift.service.strati-cdc-demo.public</name>\r\n  <value>${fs.swift.service.strati.public}</value>\r\n</property>\r\n\r\n<property>\r\n
 <name>fs.swift.service.strati-cdc-demo.region</name>\r\n  <value>RegionOne</value>\r\n</property>\r\n\r\n<property>\r\n
 <name>fs.swift.service.strati-cdc-demo.tenant</name>\r\n  <value>${fs.swift.service.strati.tenant}</value>\r\n</property>\r\n'
      compute:
        size: S
      compute-master:
        size: S
      user: # support multi-user by addding more client users.
        client-user1: # e.g. client-rzhan33
          username: user1 # e.g. rzhan33
          authorized_keys: '[""]' # copy the authorized_keys into ""
        client-user2: # e.g. client-mli014
          username: user2 # e.g. mli014
          authorized_keys: '[""]' # copy the authorized_keys into ""

scale: # Because OneOps is not standard design to update the Scaling and other common components in transition, we have to separate them here.   
    spark: # platform name 
      scaling: # don't change it 
        spark-worker: # component name
          current: '2'
          min: '2'
          max: '2'

environment: # Only support one environment for now.
  global_dns: 'true'
  availability: 'redundant'
  subdomain: 'env-spark.boo-test-spark-yliu28'
  profile: 'DEV'
  clouds: # Can support multiple clouds.
    #cloud_id: '2180660' #dev-cdc002
    dev-cdc002:
      priority: '1' # 1 or 2
      dpmt_order: '1'
      pct_scale: '100'
    #dev-cdc003:
      #priority: '1' # 1 or 2
      #dpmt_order: '1'
      #pct_scale: '100'
